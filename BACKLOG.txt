DONE
20/01/2026
* we refactored all so that droplets don't have any direct access: need either to send http requests to limited endpoints offered by node agent (port 9999 - require a key that is hash of do client) or to port 80/433 for our client's webservices 
* only ssh allowed during base snapshot creation (need ssh to install node agent)
21/01/2026
* custom snapshots are created via node agent (to send/build image) - user can then chose the snapshot (e.g. "my-llm-ready-snapshot") and do "from local/base:latest" in docker file (imag always named "base")
* REQUIRE_AUTH_ALWAYS set to true by default to force node_agent to require api key even when traffic comes from VPC
* health check revised strategy: 
  call droplet_ip:9999/{container_name}/health(port,since) (agent's endpoint)
  1) services with port: ping port and return unhealthy if no response. if yes, check container log for errors since last check -> if yes, return "degraded" else "healthy"
  2) workers without port: same but check if container is running instead of port pinging
  3) scheduled task (container) executed/run by cron: check last executed time and exit code from docker and compare against expected run time 
* undeploy revised strategy:
  we could delete droplet or container but no cleanup done afterwards (no update of nginx or A records).
  Changes implemented:
  1) User can deploy one container as usual (but now nginx updated accordingly, A record for the host ip removed incf)
  2) User can now undeploy a service (repeat teh above to all droplets, in parallel)
  We have checked that the do and cf operations use the cloud module (no direct call)
* fix stateful injection bug
* fix residual http healthcheck
* add ability to upload local SQLite db to server hosting deploy_api (to get the deployments made locally)
* to add periodic health checks we added some logic to the deploy_api.worker.py: 
  1) every 60s, ping all servers (reboot if down 5 times) and call agent/{container}/health for each service
  2) Try to restart the container after at least 3 "unhealthy" results 
  NB. to change the periodicity to say 2mns, add HEALTH_CHECK_INTERVAL=120 to the container env
  NB. Save check status to the db and purge old data (7 days retention)
* fix bad jwt cookie in frontend
* fix all messy redundant health check endpoints everywhere
22/01/2026
* Improve node agent security: no public endpoints (except ping) and api required even within VPC  
* fix some bugs: flask route n/name parameter mismatch, silent health failures, redis tcp binding 
* adding ability to see agent's log in admin tab
* using robust logic to find is service is redis
* fix health check bug (still checking docker healthcheck) and added agent's log in admin
23/01/2026
* fix list_for_workspace bug


TODO
* http://localhost:8000/api/v1/infra/agent/546131306/containers/9fe5e6_deployer_prod_redis/logs?do_token=** -> got not authenticated (maybe not passing api key?)
* maybe trim the log/what is streamed once live, for real client?
* instruct claude to pay attention to the following when modifying node agent code: The \n in the format string is being interpreted as actual newlines, breaking the string. 

TODO - UI
* Add health status display in Infrastructure view
* Add health history/timeline view
* snapshot still not fetched in background
* Deploy: when new server(s) selected, as soon as they are created, put that back to zero (to avoid re-creating in another deploy)
* redesign ui as per chatGPT mockup
* infrastructure tab: total servers and containers not updated upon deletion of any
* when building a snapshot, the background job is not replicating in all regions
* ask for token first thing (if not there yet), build snapshot automatically (if not there yet) when do token given